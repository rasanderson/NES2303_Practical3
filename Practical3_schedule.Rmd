---
title: 'Practical 3: Multiple explanatory variables'
author: "NES2303"
output:
  html_document:
    df_print: paged
    number_sections: true
  word_document:
    number_sections: true
---

<style type="text/css">
span.boxed {
  border:5px solid gray;
  border-radius:10px;
  padding: 5px;
}
span.invboxed {
  border:5px solid gray;
  padding: 5px;
  border-radius:10px;
  color: white;
}
table, td, th
{
border:0px;
}
</style>



```{r setup, include=FALSE}
library(mosaic)
library(car)
#library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
peas_dat <- read.csv("Data/peas_block.csv")
```

# Introduction
Now that we have developed a good understanding of linear models we are going to look at some more complex cases. The overall aim is to ensure that you are familiar with the use of linear models and the `lm()` function when you have multiple explanatory variables. These might be all continuous, all categorical, or a mixture of the two. Multiple explanatory variables can also arise in curve-fitting. You will also have the opportunity to gain more confidence and expertise in manipulating and plotting your data. Specific objectives are to:

1. Two categorical explanatories, with a significant interaction terms, and how to interpret them
2. One categorical and one continuous explanatory, with non-significant interaction term.
3. Fitting curves to your data with `lm()`
4. Demonstrate how to manage unbalanced designs
5. Allow you to explore some of these ideas with new datasets (optional)


# Multiple explanatory variables with interaction terms
## What are interaction terms?
Interaction terms can be useful in both designed experiments and field surveys. They provide a way of checking whether the effects of two explanatory variables on the response are independent of each other, or alternatively whether the value of one explanatory variable alters what the other one does.

## Basic concepts in the 'goal-oriented' approach
Our original linear model, with two explanatory variables is:

&nbsp;

<center>
<h2><strong><span class="boxed">lm</span> ( <span class="boxed">y</span> ~ <span class="boxed">x1</span> + <span class="boxed">x2</span> , data = <span class="boxed">mydata</span>)</strong> 
</h2></center>

&nbsp;

which can be thought of as:

$$Response = \textit{Explanatory 1} + \textit{Explanatory 2} + \epsilon$$

When we have an interaction term, our model changes to:

&nbsp;

<center>
<h2><strong><span class="boxed">lm</span> ( <span class="boxed">y</span> ~ <span class="boxed">x1</span> + <span class="boxed">x2</span> + <span class="boxed">interaction x1:x2</span>, data = <span class="boxed">mydata</span>)</strong> 
</h2></center>

&nbsp;

and we revise this to:

$$Response = \textit{Explanatory 1} + \textit{Explanatory 2} + \textit{Interaction} + \epsilon$$
 
where:

* $Response$ = dependent variable, e.g. crop growth as a result of fertiliser
* $\textit{Explanatory 1}$ = your first treatment, e.g. fertiliser (control, nitrogen, phosphorous)
* $\textit{Explanatory 2}$ = your second treatment, e.g. pesticide (control, insecticide)
* $\textit{Interaction}$ = measures how response variable changes as a result of **both** first and second treatment
* $\epsilon$ = Greek letter epsilon = the unexplained "noise" in your data
 
The $\textit{Explanatory 1}$ and $\textit{Explanatory 2}$ variables can be continuous and/or categorical. You can express an interaction term in R using the `lm()` function, by adding an extra term with the two explanatories separated by a colon `:` symbol.

Let's look at the example from the [Interactive Website on interaction terms](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-interactions-between-explanatory-variables)

## Example of interactions: blood plasma calcium in rabbits
### Before you begin
1. Start RStudio in the usual way. Open your `NES2303` project from the main menu by clicking on **File -> Open project...** and navigate to your NES2303 project file and open it. Check that the word "None" has been replaced by "NES2303" in the project listing at the top right.
2. From the main menu click "File -> New file -> R Script"
3. Save your script, e.g. call it `practical3.R`
4. Write a comment line, beginning with a `#` symbol, followed by a blank line e.g.
```{r eval =FALSE}
# Practical 3: investigate interaction terms

```

### Downloading and plotting your data
Let's look at the example of the blood Ca level in rabbits, half from a lowland agricultural farm, half from an upland farm, split according to gender. So we have two categorical explanatory variables in this example.

1. Download the file `plasma.csv` from Canvas
2. Using the File Explorer (main Windows menu) copy the `plasma.csv` file from `Downloads` and paste into your `NES2303` project folder.
3. Edit your `practical3.R` script to import the file into an R table called `plasma_dat` using the `read.csv` function.

Now see if you can produce a boxplot similar to the following. **Hints**:

* `gf_boxplot()` is main function. You'll need to add `colour = ~sex` to colour the boxes by sex
* `gf_labs()` to change default labels on x and y axes
* `gf_theme()` to change overall format of plot with the `theme_classic()` option.

Remember to save the code for your boxplot in `practical3.R` for reference!!

```{r, echo=FALSE}
plasma_dat <- read.csv("Data/plasma.csv")
gf_boxplot(calcium ~ site, colour = ~sex, data=plasma_dat) %>% 
  gf_labs(y = "Blood calcium (mg / 100 ml)", x = "Farm location") %>% 
  gf_refine(theme_classic())
```

 From the plot you can see several trends:
 
 * males seem to have lower blood Ca than females overall
 * in males the blood Ca increases from lowland to upland, whereas for females it decreases
 * the differences between males and females appear to be bigger at lowland than upland farms
 * if you were to draw a line to connect the two red boxes (females) and another line between the two blue boxes (males) the slopes of the two lines would be not be parallel.
 
 
### Create a linear model with an interaction
Using the information above, and your existing knowledge of `lm()`, try to create a linear model with soth the `site` and `sex` main effects, and also an interaction term. The resultant ANOVA table should match the one below. **Remember to include code, blank lines and comment lines in your `practical3.R` script**.
 
```{r, eval=TRUE, echo=FALSE}
calcium_lm <- lm(calcium ~ site + sex + site:sex, data=plasma_dat)
print(anova(calcium_lm))
```
```{r, eval=FALSE, echo=TRUE}
calcium_lm <- lm(calcium ~ site + sex + site:sex, data=plasma_dat)
anova(calcium_lm)
```

Notice how your ANOVA table now has **three** rows for the explanatories, namely `site` on the first row, `sex` on the second row, and the interaction term `site:sex` on the third row. When `site` and `sex` are on their own, as on the first and second row, they are referred to as **"main effects"** to distinguish them from when they both occur together in the third row as the **interaction term**. As usual there is of course a final row for the `Residuals` or unexplained noise ($\epsilon$) in your data. You can see three F-ratios (one for each explanatory variable) and associated p-values, the latter under the column headed `Pr(>F)`.

Before reading on, think about these questions:

* How would you report the above F and p-values in a report?
* Which explanatory variables are "statistically significant"?

Please speak to a demonstrator if unsure, particularly as the way in which R presents p-values is not in an acceptable format for your reports.

## How to interpret a linear model with interaction terms
When you have an interaction term as one of your explanatory variables, always look at it **first**. If it is not significant, you might be able to manage with a simpler linear model that does not include interaction terms. It is always best to try and have a simpler rather than overly complex linear model when possible.

However, in this example the interaction term **is** significant. Indeed, you have:

* `site` main effect. $F_{1,16}=0.017, p=0.899$ non-significant
* `sex` main effect. $F_{1,16}=167.658, p<0.001$
* `site:sex` interaction. $F_{1,16}=47.604, p<0.001$

So you can see that the `site:sex` interaction which indicates that `site` and `sex` are not independent of each other in their effects on blood calcium. You can also see that the `sex` main effect term is highly significant, whereas the `site` main effect is non-significant. This type of result is not uncommon, but at first glance is very confusing. Why is the `site` main effect unimportant, whilst it seems to have a big impact on blood calcium in the interaction? This seems a little contradictory.

The easiest way to understand the process is to plot individual graphs for each component. See if you can create these graphs on your own.

First of all the `site` main effect:

```{r, echo=FALSE}
gf_boxplot(calcium ~ site, data=plasma_dat)

```

It is fairly obvious that **if we ignore sex** the overall amounts of blood calcium are fairly similar in both the lowland and upland farms although the range of values is smaller in the uplands. A boxplot shows the median as the middle horizontal line (try calculating the means for comparison) and these are quite close to each other. This explains the non-significant `site` main effect.

Second, the `sex` main effect, **if we ignore site**. Produce a graph similar to the following:

```{r, echo=FALSE}
gf_boxplot(calcium ~ sex, data=plasma_dat)
```

Now the sets of values are very different, with the males much lower overall than the females, which explains the large F-ratio and highly significant (p<0.001) results for `sex` main effect.

Finally, the interaction term. As this graph is a little trickier to draw, I have included the R code. We include both the raw data, and lines connecting the means, to show the direction of change:

```{r warning=FALSE, message=FALSE}
# gf_point() adds the raw data points
# gf_line() adds lines. We use stat="summary" to indicate the mean
gf_point(calcium ~ site, colour= ~sex, data=plasma_dat) %>%
  gf_line(calcium ~ site, colour= ~sex, group= ~sex, stat="summary", data=plasma_dat)
```

The important point to note here is that **the lines are not parallel**. If the lines were parallel, either upwards or downwards, it would indicate that the blood calcium changed in a similar way for both genders when moving from lowland to upland farms. However in reality they are not parallel, indeed the differences are so big that the gradients of the lines go in opposite directions. This indicates that the physiology of male and female rabbits in response to the elevation change is not the same. When you look at plots like this:

* lines not parallel (as here): interaction is probably significant
* lines parallel (not in this example): interaction probably non-significant

To explore these data more [look at this interactive webpage](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-interactive-demonstration). This uses the same data, but you can randomly adjust some of the terms, and see how the results change. Begin by setting the interaction term to zero, and notice the difference.

# Multiple explanatories - second example
In our second example we will have one continuous explanatory, and one categorical explanatory. You will often see this type of analysis described as an ANCOVA or Analysis of Covariance. Here we look at the biochemical tissue composition (`d13C`) of fish. The categorical explanatory is the site (NW or SE) stored in `area`, and the continuous explanatory is fish length, stored in the column `standard_length`. The data are on Canvas in the file `anr_ancova.csv`. Download them into the appropriate folder, and import them into R into a `data.frame` called `anr` and run `summary(anr)` and `View(anr)` to check that it has been imported correctly.

```{r echo=FALSE}
anr <- read.csv("Data/anr_ancova.csv")
```

## Plot your data
Begin by plotting your fish data, with the area on the x-axis and biochemical composition on the y-axis. As you have two areas, you want to colour your points in accornding to the SE or NW locations, as well as add fitted lines:

```{r plot-anr, warning=FALSE, echo=TRUE}
# plot data first
gf_point(d13c ~ standard_length, col = ~area, data = anr) %>%
  gf_lm(d13c ~ standard_length, col = ~area, data = anr) %>%
  gf_labs(x = "Standard length (mm)", y = "d13c per mil") %>% 
  gf_theme(theme_classic())
```

## Analysis of covariance (ANCOVA)
When you have one continuous explanatory and one categorical explanatory, most statistics textbooks describe it as "ANCOVA". Fortunately, the method of analysis is identical to before, using the `lm` command. It is obvious that d13c increases with larger fish, and that concentrations appear to be higher in the SE than NW site. Are the lines parallel? Let's check with a linear model. One problem with having one continuous and one categorical explanatory variable is that the actual results from your `anova` function depend on the order in which you list the explanatory variables into the model. This can be resolved using the `Anova` function from the `car` library, which we will load first. 

```{r, warning=FALSE, message=FALSE}
library(car)
anr_lm <- lm(d13c ~ standard_length + area + standard_length:area, data = anr)
Anova(anr_lm) # Note capital A on Anova
```

You can see that the model shows highly significant effects from the area (SE or NW) and fish lengths with p<0.001 in both cases (remember the column headed `Pr(>F)` contains your p-values, and `e-15` indicates lots of zeros, so is a very low number). In contrast, the interaction term is non-significant with p=0.552 indicating it is unimportant. Let us therefore simplify the model and create one without the interaction term. Simpler models are better in general:

```{r}
anr_lm2 <- lm(d13c ~ standard_length + area, data = anr)
Anova(anr_lm2)
```

This second model is the one to use. Remember: look at your interaction term first, and if it is non-significant then drop it from your linear model. If, as in the rabbit example, the interaction is significant then retain all your model explanatory variables.

Why did we use `Anova()` rather than the standard `anova()` function? In this
analysis you have one continuous and one categorical explanatory variables. The
standard for of sums-of-squares (SS) used by R (Type I) are sensitive to the 
order in which you enter the explanatory variables into the model. The problem
also arises with unbalanced data if a replicate is accidentally damaged. See
the interactive website [multiple explanatories](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-aggh-lost-data) for more examples.

# Using linear models to fit curves
How can a linear model fit a curve? The term "linear model" simply refers to the way in which the different explanatory variables are combined. They are all combined using simple addition symbols `+`, for example, your model:

`calcium_lm <- lm(calcium ~ site + sex + site:sex, data=plasma_dat)`

from earlier has two `+` symbols on the right-hand side. By adjusting some terms in the explanatories you can fit curves (see below). In NES2303 we will only use linear and generalised linear models. We will not use non-linear models.

## Data transformation or polynomials?
It is much easier to fit a straight line than a curve, but often an initial exploration of your data by plotting it will reveal that a straight-line does not provide a satisfactory model. The two main approaches to resolve this are to:

1. A simple mathematical transformation of your data to bring it back to a straight line. Commonly used transformations are logarithmic `log()` and square-root `sqrt()` of your response variable.
2. Use a polynomial linear model, by adding extra explanatory variables calculated from the first explanatory variable. The explanatories thus become $x$, $x^2$, $x^3$ etc.

The use of both methods is somewhat controversial. Mathematical transformations can make it harder to understand and interpret the results, especially if there is not an obvious biological basis for the transformation. High-order polynomial terms such as $x^3$ and $x^4$ will create attractive smooth curves, but have little biological meaning. Conversely, some mathematical transformations are so common you probably never even think about them: **pH** is actually the log-transformation of hydrogen ions; these have a skewed 'log-normal' distribution by default, so taking a logarithm to create the pH scale makes sense. You can also end up with skewed data when you have counts of a response variable, since you cannot have negative counts. It used to be common to log-transform count data using `log(y+1)` (in case there are any zeros). Here we will focus on a polynomial example to fit a curve.

## Polynomial linear model: quadratic for simple curves
The word "polynomial" simply indicates that we are going to derive new $x^2$ (x-squared), $x^3$ (x-cubed) etc. terms to put into our linear model in order to fit a curve. Due to problems of biological interpretation, it is very rare to go beyond a simple **quadratic** model:

$$\textit{Response variable}=\textit{Explanatory variable}+\textit{Explanatory variable}^2 + \epsilon$$
where

* $\textit{Response variable}$ = dependent, y-axis variable
* $\textit{Explanatory variable}$ and $\textit{Explanatory variable}^2$ = independent, x-axis variable, either in its original form, or squared.
* $\epsilon$ = Greek letter epsilon for unknown noise or variation in your data.

These are easy to create using the `lm()` function using:

`lm(response ~ explanatory + I(explanatory^2), data=dataset_name)`

The slightly confusing change to the usual syntax is that `^2` is used to indicate that we are going to add a squared explanatory, and we have to "wrap" the squared term in an extra set of brackets prefaced by `I()` to indicate that we "Intend" to do this, and it isn't a typing error.

## _Lolium_ growth example
Let us build on the example at the [Interactive Website on Curve fitting](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-fitting-curves) for growth of the grass _Lolium perenne_ in soils of different water content. You will need to first:

* dowload the file `grass_growth.csv` from Canvas
* copy `grass_growth.csv` into your `Data` folder
* import it into R using `read.csv()` as a table called `lolium_dat`

Use the `head()` function to look at the first few rows, or double-click on `lolium_dat` in the "Environment" pane (top-right of RStudio) to view the whole table of data. Next, produce a scatterplot of the data:

```{r, echo=FALSE}
lolium_dat <- read.csv("Data/grass_growth.csv")
```

```{r, echo=TRUE}
gf_point(growth ~ water, data=lolium_dat) %>% 
  gf_labs(x = "Soil water content (%)", y="Growth rate") %>% 
  gf_theme(theme_classic())
```

You can see that whilst the pattern of points is broadly increasing, but beyond about 75% soil water it starts to drop sharply. _Lolium_ is not an aquatic plant, so this makes sense biologically of course. 

### Simple (straight line) linear model
Let's look at a simple straight-line linear model to begin. This model is of the form:

$growth = water + \epsilon$

```{r, echo=TRUE}
lolium_lm1 <- lm(growth ~ water, data = lolium_dat)
summary(lolium_lm1)
```

So that was easy! You have a highly significant linear model, showing that _Lolium_ growth increases with water content. The overall model is significant with $F_{1,17}=11.45, p=0.004$, and the $R^2=0.367$ (model explains 36.7% of variation). But... we should always check model assumptions by looking at the residuals to see if there are any abnormalities.

### Diagnostics for simple (straight line) linear model
We can actually produce 4 "diagnostic" plots quite easily with the `plot()` function, writing `plot(lolium_lm1)`. Only the first two are really useful here. The `plot()` function extracts the residuals and creates a QQ plot automatically. All the plots look at "Residuals". Remember residuals are the differences between your fitted straight line and the observed points.

```{r, echo=TRUE, eval=TRUE}
plot(lolium_lm1, which=1:2) # We only want the first two diagnostic plots
```

* The first plot is called **Residuals vs Fitted**. Notice how it bends down at both low and high fitted values, with the residuals drifting away from zero, whereas ideally the scatter around should be fairly constant.
* The second plot, titled **Normal Q-Q** is the quantile-quantile plot you have created manually before. Notice how poorly the points align with the expected dotted line, especially at the extremes.

### Add the fitted line to see the problem
If you modify your plot, by adding a line via the `gf_lm()` function, the problem is obvious:

```{r, echo=TRUE}
gf_point(growth ~ water, data=lolium_dat) %>% 
  gf_labs(x = "Soil water content (%)", y="Growth rate") %>% 
  gf_lm() %>% 
  gf_theme(theme_classic())
```

Thus, by looking at the raw data, especially when you overlay the fitted line, as well as the diagnostic residual plots from the model, as well as basic plant biology, you can see this is not a good model. Let's change it to a curve, by changing it to a quadratic.

### Quadratic (curve) linear model
This means our model is now of the form:

$growth = water + water^2 + \epsilon$

which we can express in R as:

```{r, echo=TRUE}
lolium_lm2 <- lm(growth ~ water + I(water^2), data = lolium_dat)
summary(lolium_lm2)
```

This linear model is also significant. You will notice that the overall model is highly significant with $F_{2,16}=64.37,p<0.001$ and $R^2=0.876$, or 87.6% explained. **Note** In the previous sentence the p-value is written as $p<0.001$ which indicates "p is less than 0.001". This is because in the overall linear model output it the p-value is shown as `p-value: 2.23e-08` which is the same as $p=0.0000000223$. The convention is that for tiny p-values, the actual amount is not shown, and instead the "less than 0.001" syntax is used.

Is the improvement in the overall model of `lolium_lm2` compared to the earlier `lolium_lm1` significant? We can compare the two models using `anova()`:

```{r, eval=FALSE, echo=TRUE}
anova(lolium_lm1, lolium_lm2)
```
```{r, eval=TRUE, echo=FALSE}
print(anova(lolium_lm1, lolium_lm2))
```


This is a test to compare our two linear models, and it gives $F_{2,16}=70.48, p<0.001$ indicating that there is a big difference in the performance of our two models, with our second one, labelled `Model 2` in the above output, better. Notice also the column headed `RSS` which stands for "Residual Sum of Squares". This is a measure of the "noise" that is not explained by our model. You can see that it is much smaller at 146.81 for the second model.

### Coefficients from quadratic model `lolium_lm2`
Look at the column headed `Estimate` in the `summary(lolium_lm2)` above. All three estimates, the `(Intercept)`, the `water` and quadratic `I(water^2)` terms are significant, but their values are very different, at 10.716, 1.067 and -0.009 respectively. It is common for the absolute value of the squared term to be very small. **Questions**:

* Why is the value of the estimate for the squared term so small? Why might this be? 
* Why does the squared term have a negative value?
* How does the value of `water^2` change as you square the value of water?

Ask on Microsoft Teams if unsure, under the "Section 3 - Dealing with multiple explanatories" channel.

### Diagnostics and plots from quadratic model `lolium_lm2`
Replot the first two diagnostic from `lolium_lm2` and you can see that both of these are improved compared to the original.

```{r, echo=FALSE}
plot(lolium_lm2, which=1:2)
```

Finally, let's plot the raw data, plus our quadratic model with confidence intervals. By default, the `gf_lm()` function adds a straight line, but if we give it the same quadratic we used in the model, it will add a curve.

```{r}
gf_point(growth ~ water, data = lolium_dat) %>% 
  gf_lm(formula = y ~ x + I(x^2), interval = "confidence")
```

This looks much better and more realistic. It is a better statistical model, and also makes more biological sense. Before you go further, try modifying this plot so that the horizontal axis is labelled "Soil wated %" and the vertical axis "Growth rate".


# Take home messages from today's practical
These are the key things you should have learnt:

1. How to measure whether two (or more) explanatory variables are independent in their effects via interaction terms
2. How to fit curves using a quadratic model
3. Remember to use the `Anova()` rather than `anova()` function for ANCOVA.

If you have any questions about these four topics, pop them into Microsoft Teams.


# How to improve your experimental design (in your own time)
Good experimental design will improve your ability to detect treatment effects. Recall from [Interactive Website on Blocking](https://naturalandenvironmentalscience.shinyapps.io/expt_design/) that we use block designs to increase our ability to detect a treatment effect. In the linear models we have looked at so far, there has only been one explanatory variable, so we can think of it as:

$$Response = Explanatory + \epsilon$$
where:

* $Explanatory$ = your treatment, e.g. fertiliser (control, nitrogen, phosphorous)
* $Response$ = dependent variable, e.g. crop growth as a result of fertiliser
* $\epsilon$ = Greek letter Epsilon which stands for the unknown noise or variation in your data

To determine if your explanatory variable has an effect on the response you must **separate the signal from the noise**. Thus if the unknown random variation in your data, $\epsilon$, is too big, then you won't be able to know if your explanatory affects the response.

## Fully randomised design
In a field or laboratory experiment a common design is the **fully randomised design**. For example, you might have:

* 4 types of antibiotic being tested. You have 4 replicate Petri dishes for each antibiotic, and you randomly assign each antibiotic to one of the Petri dishes. You put all 15 Petri dishes on a tray into the incubator.
* 4 varieties of tomato plant are being grown, and you want to measure the chlorophyll content of their leaves. You grow up 4 replicate seedlings of each variety, randomly put them into different pots in a glasshouse
* 3 fertilisers plus a control (4 treatment levels) and you want to understand their effects on growth of pea plants. You allocate the fertilisers at random and later assess the growth of the pea plants.

Now there is nothing wrong with these fully randomised designs, but the problem is that **they are at greater risk of going wrong**. Look at the following setup: you have 4 replicates of 4 treatments, which we'll label A, B, C and D. They are assigned at random:

![](images/full_randomised.png)

Purely by chance, most of treatment A is on the right, and most of treatment C on the left. Now transfer this to your experiments:

* The above might represent a plastic tray with Petri dishes in an incubator. The right is next to the door of the incubator. Petri dishes with Treatment A will get more variable temperatures as the door is opened.
* The above might represent your glasshouse. The right might be in the shade, the left in open sun. Tomatoes with Treatment C will have warmer temperatures.
* The above might represent your plots with pea plants. There might be a wet patch of waterlogged soils on the right of the field. Peas receiving treatment A will grow less.

This is going to make it hard to separate the signal from the noise. The unexplained error (noise) will be large and might swamp your analysis, so you can't draw any conclusions:

![](images/large_error.png)

## Randomised block design
There might be natural variation in your "experimental arena" whether it is a laboratory, glasshouse or field experiment. A fully randomised design is vulnerable to this variation biasing your results. To solve the problem we divide the experimental arena into blocks, and randomly allocate our replicate treatments within each block:

* Divide your laboratory tray into 4 strips with marker pen, arranged so that when you put the tray into the incubator one strip is near the door, the 4th strip near the back of the incubator. Have 4 Petri dishes in each strip, and randomly allocate one of your 4 antibiotics to each of the four Petri dishes.
* Divide your glasshouse into 4 bands from sunny to shady. Within each band, randomly place 4 plant pots containing your four different varieties of tomato.
* Divide your plot experiment with your peas into 4 areas, from the waterlogged soil to the drier soil. Within each area, mark out 4 plots, and randomly allocate the different fertiliser treatments.

Now your design looks like this diagram:

![](images/randomised_block.png)

Here the blocks are numbered 1 to 4. Within each block the treatments A, B, C & D are allocated at random. This removes biases due to incubator doors, glasshouse shade or waterlogged fields.

## How does a randomised block design help?
You what to know whether your explanatory variable has an effect on your response, by separating the signal from the noise. The advantage of a randomised block design is that you correct for the environmental or experimental variation in your blocks. Then your model becomes:

![](images/small_error.png)

Some of the original variation has now gone into the block effect. This reduces the unknown variation (error), so you are **more** likely to detect any relationship between the response and explanatory.

In algebraic terms your linear model is now:

$$Response = Explanatory + Block + \epsilon$$

Where you include an explicit term called $Block$ into your linear model

# Example of a blocked experiment
We will start by using some data that comes from the pea growth and fertiliser experiment mentioned above. The data are available on Canvas in the `peas_block.csv` file and use the `read.csv()` function to import it into `peas_dat`. The data are in "long" format, with different columns for the block, fertiliser type and yield. The data are not arranged in a "wide" grid as in the field (like the diagrams above).

When doing your own projects, wide format might be how you enter it in Excel. Long format is how you analyse it. you'll be pleased to know it's easy to convert between "long" and "wide" formats in R. See the interactive website for this practical for details if interested.

![](images/Pivot_longer.png)

## Initial exploration of pea data
Do some initial explorations of your peas data. You should be able to calculate some simple statistics and visualise it before going ahead with an analysis via a linear model. Do some initial queries to check:

* Overall mean yield across all the plots; use
  + `summary(peas_dat)` which also gives minimum, maximum etc., 
  + `mean(Yield ~ NULL, data=peas_dat)` or
  + `mean(~Yield, data=peas_dat)`
* Standard deviation of `Yield`

**Question** When you used the `summary()` function, what did it return for the `Block` variable and the `Treatment` variable? Does a mean, minimum and maximum value for your `Block` variable make any sense?? How might it be better to record the `Block` variable?

Produce a boxplot similar to this using the `gf_boxplot()` function, combined with `%>%`, `gf_labs()` and `gf_refine()` with `theme_classic()`. Build it up line-by-line, from a basic boxplot, to the more attractive one below:

```{r, echo=FALSE}
gf_boxplot(Yield ~ Treatment, data=peas_dat) %>% 
  gf_labs(x = "Experimental treatment", y = "Yield of peas per plot (g)") %>% 
  gf_refine(theme_classic())

```


## Linear model (two-way ANOVA) of the pea data
### Recoding `Block` and `Treatment`
You may have noticed that when you ran the `summary()` function earlier it returned the minimum, maximum, average etc. of your `Block` variable. This is of course nonsensical. You could just as easily have coded your six blocks A, B, C, D, E and F. Unfortunately, if you leave `Block` unchanged, R will assume it is a continuous variable, and that `Block 6` is "bigger" than `Block 1`. Likewise, the "character" class for `Treatment` is a little confusing, as it does not show the individual fertiliser treatments. We can re-code both variables into factors, and re-run the `summary()` function to see the difference. Remember that the `$` symbol allows us to access individual columns in a table ("data frame") and the `as.factor()` function tells R that the variable is categorical:

```{r}
peas_dat$Block <- as.factor(peas_dat$Block)
peas_dat$Treatment <- as.factor(peas_dat$Treatment)
summary(peas_dat)
```

That is much better. We no longer have silly statistics for the `Block` column, and the individual fertiliser treatments are now being displayed for the `Treatment`. What do the numbers `4` and `6` represent alongside each row of these entries?

### Linear models of pea dataset with and without `Block`
We'll begin with a simple linear model that ignores the `Block` variable, just to demonstrate the improvement afterwards. This is a one-way ANOVA, as there is only one explanatory categorical variable. It is equivalent to analysing your data as a fully-randomised design.

```{r}
pea_dat_lm1 <- lm(Yield ~ Treatment, data=peas_dat)
print(anova(pea_dat_lm1))
```

What do you conclude from this? Do the fertilisers have a significant effect? How would your report these data? Remember that by convention p=0.05 is usually the critical cut-off for "statistical significance" so what would be an appropriate way to report your results? Do a QQ plot of the residuals? Are assumptions of the model robust?

Now repeat but include `Block`. Notice that we are naming our second set of results `pea_dat_lm2`. This is a common convention in R, to end the name of the output with the name of the analysis (here `lm`) and if doing several models to add a suffix `1`, `2` etc. for clarity:


```{r, echo=FALSE}
pea_dat_lm2 <- lm(Yield ~ Treatment + Block, data=peas_dat)
print(anova(pea_dat_lm2))
```
 
Notice how the `Treatment` for your fertilisers is now significant. There are several points to notice here:

* The total Sums of Square (a measure of total variation in your dataset) in the data is unchanged in both `lm1` and `lm2` of your pea yield. See [This Interactive Website](https://naturalandenvironmentalscience.shinyapps.io/how_anova_works/#section-variances-with-categorical-explanatory-variables) to remind yourself of the different components of SS
* The **Residuals** are **much smaller** in `pea_dat_lm2`. Residuals are unexplained noise, or the $\epsilon$ in your data. The smaller you can make the noise, the more likely you are to detect a treatment effect.
* The SS is the same for your `Treatment` in both `pea_dat_lm` and `pea_dat_lm2`
* Variance explained by each term is measured in the column headed `Mean Sq` by dividing the `Sum sq` by `Df` (degrees of freedom)
* The `F value` for the fertiliser treatment is calculated by dividing its `Mean Sq` by that of the Residuals. The bigger the F-ratio the more significant (lower p-value), i.e.
   + F-ratio = Treatment Mean Sq / Residual Mean sq
   + `pea_dat_lm1`  F-value = 86.427 / 30.251 = 2.857 (non-significant)
   + `pea_dat_lm2`  F-value = 86.427 / 17.349 = 4.982 (significant)

So what changed was that we moved some of the variation in your data out of "noise" and into "Block". This allowed us to focus on what we were interested in, namely the fertiliser treatment. As before, check the residuals of your second linear model, and create a QQ plot.

## Which fertiliser treatment is best?
Of course, the above analysis merely shows us that fertiliser has an effect on yield. We also want to know how one fertiliser compares with the others, as well as the Control. So we need to do a Tukey multiple comparison test, using the `TukeyHSD()` function. Try running the `TukeyHSD()` function on the `pea_dat_lm2` results. You'll see that it gives a huge amount of results, because not only does it compare all the pairwise Fertiliser combinations, it also does the same for all the Blocks. We are not interest in Block pairwise comparisons, so we can force the function to only look at the fertilisers via:

```{r, eval=FALSE}
TukeyHSD(pea_dat_lm2, which="Treatment")
```

Next, create a plot of the results of the Tukey test. Ask a demonstrator or member of staff if you are unsure how to interpret the results.
